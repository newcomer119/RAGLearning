{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "83d0b737",
      "metadata": {},
      "source": [
        "BUILDING A CHATBOT\n",
        "\n",
        "Conversation RagChatbot : Enable a Chatbot experience over an external source of data \n",
        "Agents : Build a Chatbot that takes action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "bab5a9d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os \n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "9464e88e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 8192, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x000001D48699F160>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001D48699DCF0>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "# gemma2-9b-it was decommissioned; use a supported model (see https://console.groq.com/docs/models)\n",
        "model = ChatGroq(model=\"llama-3.1-8b-instant\", groq_api_key=groq_api_key)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "cfe78311",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Nice to meet you, Mitarth Pandey! It's great to hear that you're aspiring to become a software engineer. That's a fantastic career goal, and with dedication and hard work, you can achieve it.\\n\\nAs a software engineer, you'll have the opportunity to work on a wide range of technologies, from mobile apps to web applications, and from artificial intelligence to data science. You'll have the chance to solve complex problems, collaborate with a team, and continuously learn new skills.\\n\\nTo help you on your journey, I'd like to ask a few questions:\\n\\n1. What programming languages are you interested in learning?\\n2. Do you have any experience with coding or software development?\\n3. What areas of software engineering interest you the most (e.g., web development, mobile app development, data science, etc.)?\\n4. Are you planning to pursue a degree in computer science or software engineering, or do you want to learn through online courses and bootcamps?\\n\\nLet me know your answers, and I'll be happy to provide you with guidance, resources, and advice to help you achieve your goal of becoming a software engineer!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 231, 'prompt_tokens': 52, 'total_tokens': 283, 'completion_time': 0.327068298, 'completion_tokens_details': None, 'prompt_time': 0.002542032, 'prompt_tokens_details': None, 'queue_time': 0.045759848, 'total_time': 0.32961033}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c4cfb-b582-7673-bb3b-7e031aac6400-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 52, 'output_tokens': 231, 'total_tokens': 283})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "model.invoke([HumanMessage(content=\"Hi, My Name is Mitarth Pandey and i am aspiring to become software engineering\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "id": "b2db4623",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is Mitarth Pandey.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 311, 'total_tokens': 320, 'completion_time': 0.013547868, 'completion_tokens_details': None, 'prompt_time': 0.017740991, 'prompt_tokens_details': None, 'queue_time': 0.045004429, 'total_time': 0.031288859}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c4cfb-b78d-71c3-8e20-9fd45bca987d-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 311, 'output_tokens': 9, 'total_tokens': 320})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.messages import AIMessage\n",
        "model.invoke(\n",
        "    [\n",
        "        HumanMessage(content=\"Hi,My Name is Mitarth Pandey and i am aspiring to become software engineering\"),\n",
        "        AIMessage(content=\"Nice to meet you, Mitarth Pandey. It\\'s great to hear that you\\'re aspiring to become a software engineer. That\\'s a fantastic career goal, and with dedication and hard work, you can achieve it.\\n\\nAs a software engineer, you\\'ll have the opportunity to work on a wide range of projects, from developing mobile apps to creating complex enterprise software systems. You\\'ll need to have strong programming skills, problem-solving abilities, and a passion for learning new technologies.\\n\\nTo help you get started, here are some general tips:\\n\\n1. **Learn the fundamentals**: Start by learning the basics of programming, including data structures, algorithms, and object-oriented programming.\\n2. **Choose a programming language**: Select a language you enjoy working with, such as Python, Java, or JavaScript.\\n3. **Practice, practice, practice**: Build small projects and contribute to open-source projects to gain practical experience.\\n4. **Stay up-to-date with industry trends**: Follow industry leaders, blogs, and news outlets to stay current with the latest technologies and advancements.\\n5. **Network with other engineers**: Join online communities, attend coding meetups, and connect with other engineers to learn from their experiences.\"),\n",
        "        HumanMessage(content=\"What is my name what is it I do ?\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f504df46",
      "metadata": {},
      "source": [
        "### MESSAGE HISTORY\n",
        "\n",
        "We can use a message history class to wrap our model and make it stateful. This will keep track of inputs and outputs of the model, and store them in some datastore\n",
        "Future Interactions will then load these messages and pass them into the chain as part of the input. Lets see how to do this "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "0934a3fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "store = {}\n",
        "\n",
        "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "\n",
        "with_message_history = RunnableWithMessageHistory(model,get_session_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "732e21ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "config = {\"configurable\" : {\"session_id\" : \"chat1\"}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "id": "e08e4307",
      "metadata": {},
      "outputs": [],
      "source": [
        "response = with_message_history.invoke(\n",
        "    [HumanMessage(content=\"Hi,My Name is Mitarth Pandey and i am aspiring to become software engineering\")],\n",
        "    config=config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "id": "0981c2df",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Hello Mitarth Pandey,\\n\\nIt's great to meet you. Becoming a software engineer can be a rewarding and challenging career path. With the increasing demand for technology and digital solutions, the field of software engineering has a lot of opportunities.\\n\\nTo support your aspiration, I'd like to know more about your background and interests. Here are a few questions to get started:\\n\\n1. What programming languages are you familiar with, and which ones do you want to learn?\\n2. What areas of software engineering interest you the most, such as web development, mobile app development, artificial intelligence, or data science?\\n3. Do you have any prior experience in software development, or are you starting from scratch?\\n4. What kind of projects or activities have you done in the past that you enjoyed or found challenging?\\n5. Are you planning to pursue a degree in computer science or software engineering, or do you want to learn through online resources and self-study?\\n\\nFeel free to share any information that you think might be helpful for me to understand your goals and aspirations. I'll do my best to offer guidance and support.\""
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "bb52e295",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is Mitarth Pandey.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 290, 'total_tokens': 299, 'completion_time': 0.009726433, 'completion_tokens_details': None, 'prompt_time': 0.020090439, 'prompt_tokens_details': None, 'queue_time': 0.046249161, 'total_time': 0.029816872}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c4cfb-ba2d-7d51-bd77-94b4a49dfe1b-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 290, 'output_tokens': 9, 'total_tokens': 299})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with_message_history.invoke(\n",
        "    [HumanMessage(content=\"What is my name?\")],\n",
        "    config=config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "a8e7fc71",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"I don't have any information about your name. I'm a large language model, I don't have the ability to retain information about individual users, and our conversation has just started, so I don't have any context about who you are. If you'd like to share your name, I'm happy to chat with you!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 67, 'prompt_tokens': 41, 'total_tokens': 108, 'completion_time': 0.081120799, 'completion_tokens_details': None, 'prompt_time': 0.003114603, 'prompt_tokens_details': None, 'queue_time': 0.046388416, 'total_time': 0.084235402}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c4cfb-bac1-75b1-8bb4-3b015e7e4e25-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 41, 'output_tokens': 67, 'total_tokens': 108})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Change the config --> sessionid\n",
        "config1={\"configurable\" : {\"session_id\" : \"chat2\"}}\n",
        "with_message_history.invoke(\n",
        "    [HumanMessage(content=\"What is my name ? \")],\n",
        "    config = config1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "id": "d2ee5d7c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Nice to meet you, Mitarth Pandey. It's great that you're willing to share your name with me. I'll remember it for our conversation, but as I mentioned earlier, I don't retain information about individual users, so don't worry about me knowing it outside of this chat. How's your day going so far?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 69, 'prompt_tokens': 126, 'total_tokens': 195, 'completion_time': 0.097144578, 'completion_tokens_details': None, 'prompt_time': 0.010905087, 'prompt_tokens_details': None, 'queue_time': 0.047028453, 'total_time': 0.108049665}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c4cfb-bb8f-7d91-883f-136a564c8d24-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 126, 'output_tokens': 69, 'total_tokens': 195})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Change the config --> sessionid\n",
        "config1={\"configurable\" : {\"session_id\" : \"chat2\"}}\n",
        "with_message_history.invoke(\n",
        "    [HumanMessage(content=\"Hey my name is Mitarth Pandey  \")],\n",
        "    config = config1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db2ef6cc",
      "metadata": {},
      "source": [
        "## PROMPT TEMPLATES \n",
        "\n",
        "\n",
        "Prompt templates help to turn raw user information into a format that LLM can work with . in this case the raw user input is just a message, which we are passing to LLM\n",
        ".Lets Now that a bit more complicated First Let add in a system message with some custom instructions(but still taking message as input ). Next we will add in more input besides just the messages "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "d861c8fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant that helps to answer questions best to your ability\"),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "id": "1ff72585",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Nice to meet you, Mitarth. I'm happy to help you with any questions or topics you'd like to discuss. How's your day going so far? Is there something specific on your mind that you'd like to talk about, or would you like me to suggest some conversation starters?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 60, 'prompt_tokens': 55, 'total_tokens': 115, 'completion_time': 0.080220442, 'completion_tokens_details': None, 'prompt_time': 0.002670112, 'prompt_tokens_details': None, 'queue_time': 0.045226937, 'total_time': 0.082890554}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c4cfb-bc7b-7403-957f-de176125ff76-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 55, 'output_tokens': 60, 'total_tokens': 115})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"messages\" : [HumanMessage(content=\"Hi my name is Mitarth\")]})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "06982ab5",
      "metadata": {},
      "outputs": [],
      "source": [
        "with_message_history = RunnableWithMessageHistory(chain,get_session_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "id": "a22fc588",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"I'm happy to help, but I don't have any information about your personal identity. I'm a conversational AI assistant, and our conversation just started, so I don't have any prior knowledge about you. If you'd like to share your name with me, I'd be happy to chat with you. Alternatively, I can generate a random name if you'd like a fun and fictional identity. Just let me know!\""
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config = {\"configurable\" : {\"session_id\" : \"chat3\"}}\n",
        "response = with_message_history.invoke(\n",
        "    [HumanMessage(content=\"What is my name\")],\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "response.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "id": "0d15c37d",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Add more complexity \n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all the questions to best of your ability in {language}\"\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "chain = prompt | model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "id": "0912d91b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'नमस्ते Mitarth, मैं आपकी सहायता करने के लिए यहां हूं। क्या मैं आपकी किसी समस्या का समाधान करने में आपकी मदद कर सकता हूं?'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = chain.invoke({\"messages\" : [HumanMessage(content=\"Hi My name is Mitarth\")], \"language\" : \"Hindi\"})\n",
        "response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4663018b",
      "metadata": {},
      "source": [
        "Lets now wrap this more complicated chain in a Message History class. this time , because there are multiple keys in the input, we need to specify the correct key to use to save the chat history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "e514cc3f",
      "metadata": {},
      "outputs": [],
      "source": [
        "with_message_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"messages\",\n",
        "    history_messages_key=\"messages\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "id": "b10ad980",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'मैं आपकी सहायता करने के लिए तैयार हूँ। कृपया कोई प्रश्न पूछें, मैं आपको संभव से अधिक जानकारी प्रदान करने का प्रयास करूँगा।'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set up the config for this conversation session\n",
        "config = {\"configurable\" : {\"session_id\" : \"chat4\"}}\n",
        "\n",
        "# Store information in the conversation\n",
        "response = with_message_history.invoke(\n",
        "    {'messages' :[HumanMessage(content=\"my name is Mitarth Pandey\")],\"language\" : \"Hindi\"},\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "response.content   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "id": "3e5f2753",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "''"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Ask a question - the chatbot will remember previous messages from this session\n",
        "message = with_message_history.invoke(\n",
        "    {'messages' :[HumanMessage(content=\"What's my name?\")],\"language\" : \"Hindi\"},\n",
        "    config=config,\n",
        ")\n",
        "message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "441d9743",
      "metadata": {},
      "source": [
        "## MANAGE THE CONVERSATION"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94385497",
      "metadata": {},
      "source": [
        "One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded \n",
        "and potentially overflow the context window of the LLM. Therefore it is important to add a step limits the size of message you are passing in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "id": "a642bdcd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\87504\\Desktop\\RAGLearning\\.venv\\lib\\site-packages\\langchain_core\\language_models\\base.py:336: UserWarning: Using fallback GPT-2 tokenizer for token counting. Token counts may be inaccurate for non-GPT-2 models. For accurate counts, use a model-specific method if available.\n",
            "  return len(self.get_token_ids(text))\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "c:\\Users\\87504\\Desktop\\RAGLearning\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\87504\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[SystemMessage(content='Hi I am a good assistant', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=\"Hi I'm Bob\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='hi!', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
              " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='nice', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
              " HumanMessage(content='What is 2+2', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='4', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
              " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='no problem', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
              " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='yes!', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.messages import SystemMessage,trim_messages\n",
        "trimmer = trim_messages(\n",
        "    max_tokens = 70,\n",
        "    strategy = \"last\",\n",
        "    token_counter=model,\n",
        "    include_system=True,\n",
        "    allow_partial = False,\n",
        "    start_on=\"human\"\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Hi I am a good assistant\"),\n",
        "    HumanMessage(content=\"Hi I'm Bob\"),\n",
        "    AIMessage(content=\"hi!\"),\n",
        "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
        "    AIMessage(content=\"nice\"),\n",
        "    HumanMessage(content=\"What is 2+2\"),\n",
        "    AIMessage(content=\"4\"),\n",
        "    HumanMessage(content=\"thanks\"),\n",
        "    AIMessage(content=\"no problem\"),\n",
        "    HumanMessage(content=\"having fun?\"),\n",
        "    AIMessage(content=\"yes!\")\n",
        "]\n",
        "\n",
        "trimmer.invoke(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "id": "9fd5bb5f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'You like vanilla ice cream.'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "chain = (\n",
        "    RunnablePassthrough.assign(message=itemgetter(\"messages\") | trimmer) | prompt | model\n",
        ")\n",
        "response = chain.invoke({\n",
        "    \"messages\" : messages + [HumanMessage(content=\"What ice cream do I Like\")],\n",
        "    \"language\" : \"English\" \n",
        "})\n",
        "\n",
        "\n",
        "response.content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "id": "f16aedb3",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Lets Wrap this in Message History \n",
        "\n",
        "with_message_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,\n",
        "    input_messages_key =\"messages\",\n",
        ")\n",
        "\n",
        "config = {\"configurable\":{\"session_id\" : \"chat5\"}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "id": "7f1e9cab",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Your name is Bob.'"
            ]
          },
          "execution_count": 137,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = with_message_history.invoke(\n",
        "    {\n",
        "        \"messages\"  : messages + [HumanMessage(content=\"What is my name\")],\n",
        "        \"language\" : \"English\",\n",
        "    },\n",
        "    config = config,\n",
        ")\n",
        "response.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
